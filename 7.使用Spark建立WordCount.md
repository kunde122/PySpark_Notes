[TOC]

> 使用map与reduceByKey编写Spark版本的WordCount

```python
# 读取文本
textFile = sc.textFile(r"C:\Code\Python\Data\test.txt")

# 读取每一个文字
stringRDD = textFile.flatMap(lambda x: x.split(' '))

# map reduce 计算每一个文字出现的次数
countsRDD = stringRDD.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)

# 存储计算结果
countsRDD.saveAsTextFile(r'C:\Code\Python\Data')
```
