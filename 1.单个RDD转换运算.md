[TOC]

# 1.建立RDD

> 最简单的建立RDD的方法就是使用SparkContext的parallelize方法

> `intRDD = sc.parallelize([3, 1, 2, 4, 5])`

# 2.RDD => List

> `intRDD.collect()`

> [3, 1 ,2 , 4, 5]

# 3.map运算

通过传入的函数将每一个元素经过函数运算产生另一个RDD

## 3.1 map使用具名函数

```python
def addOne(x):
    return x+1

intRDD.map(addOne).collect()
```

> [4, 2, 3, 5, 6]

> ## 3.2 map使用匿名函数

> ```python
> intRDD.map(lambda x: x+1).collect()
> ```

> [4, 2, 3, 5, 6]

> # 4.filter运算

> 对RDD内每一个元素进行筛选并产生另外的RDD

> ```python
> intRDD.filter(lambda x: x>3).collect()
> ```

> [1, 2]

# 5.distinct运算

去除RDD中重复的元素

```python
intRDD2 = sc.parallelize([1, 2, 3, 4, 1, 2, 3, 4])
intRDD2.distinct().collect()
```

> [1, 2, 3, 4]

# 6.randomSplit运算

将整个集合元素, 以随机的方式按照比例分为多个RDD

```python
# 将整个集合按照4:6的比例分为2个RDD
sRDD = intRDD.randomSplit([0.4, 0.6]) # 返回一个包含两个RDD的列表
sRDD[0].collect()
sRDD[1].collect()
```

> [3, 1] [2, 4, 5]

# 7.groupBy运算

根据传入的函数规则, 将数据分为多个迭代器(以前版本是list,现在改版了是迭代器)

```python
gRDD = intRDD.groupBy(lambda x: 'even' if (x%2==0) else 'odd').collect()
print gRDD
```
