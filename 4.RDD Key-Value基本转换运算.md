[TOC]

> Spark RDD支持键值运算(Key-Value), 该运算是Map/Reduce的基础

```python
# 新建一个 Key-Value RDD
kvRDD1 = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2)])
kvRDD1.collect()

# 列出全部Key值(第一个栏位)
kvRDD1.keys().collect()

# 列出全部Value值(第二个栏位)
kvRDD1.values().collect()

# 使用filter筛选
kvRDD1.filter(lambda x: x[0] < 4).collect()

# mapValues运算: mapValues运算可以针对RDD内每一组(key, value)的value进行运算, 并产生另一个RDD
kvRDD1.mapValues(lambda value: value*value).collect()

# sortByKey: 按照key从小到大排序, 令参数ascending=False则从大到小排序
kvRDD1.sortByKey().collect()
kvRDD1.sortByKey(ascending=False).collect()

# reduceByKey: 根据Key值进行reduce运算
kvRDD1.reduceByKey(lambda x, y: x + y).collect()
```

> [(3, 4), (3, 6), (5, 6), (1, 2)] [3, 3, 5, 1] [4, 6, 6, 2] [(3, 4), (3, 6), (1, 2)] [(3, 16), (3, 36), (5, 36), (1, 4)] [(1, 2), (3, 4), (3, 6), (5, 6)] [(5, 6), (3, 4), (3, 6), (1, 2)] [(1, 2), (3, 10), (5, 6)]
